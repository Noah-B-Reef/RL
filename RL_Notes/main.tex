\documentclass{article}

\usepackage{amsmath, amsthm, amssymb, amsfonts}
\usepackage{thmtools}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage{geometry}
\usepackage{float}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{framed}
\usepackage[dvipsnames]{xcolor}
\usepackage{tcolorbox}

\colorlet{LightGray}{White!90!Periwinkle}
\colorlet{LightOrange}{Orange!15}
\colorlet{LightGreen}{Green!15}

\newcommand{\HRule}[1]{\rule{\linewidth}{#1}}

\declaretheoremstyle[name=Theorem,]{thmsty}
\declaretheorem[style=thmsty,numberwithin=section]{theorem}
\tcolorboxenvironment{theorem}{colback=LightGray}

\declaretheoremstyle[name=Proposition,]{prosty}
\declaretheorem[style=prosty,numberlike=theorem]{proposition}
\tcolorboxenvironment{proposition}{colback=LightOrange}

\declaretheoremstyle[name=Principle,]{prcpsty}
\declaretheorem[style=prcpsty,numberlike=theorem]{principle}
\tcolorboxenvironment{principle}{colback=LightGreen}

\setstretch{1.2}
\geometry{
    textheight=9in,
    textwidth=5.5in,
    top=1in,
    headheight=12pt,
    headsep=25pt,
    footskip=30pt
}

% ------------------------------------------------------------------------------

\begin{document}

% ------------------------------------------------------------------------------
% Cover Page and ToC
% ------------------------------------------------------------------------------

\title{ \normalsize \textsc{}
		\\ [2.0cm]
		\HRule{1.5pt} \\
		\LARGE \textbf{\uppercase{Reinforcement Learning Notes}
		\HRule{2.0pt} \\ [0.6cm] \LARGE{Overview} \vspace*{10\baselineskip}}
		}
\date{}
\author{\textbf{Author} \\ 
		Noah Reef\\
		UT Austin \\
		Spring 2025}

\maketitle
\newpage

\tableofcontents
\newpage

% ------------------------------------------------------------------------------

\section{Introduction}
We can think of Reinforcement Learning as a separate paradigm of machine learning form other popular forms of machine learning (e.g. Supervised and Unsupervised learning) whose goal is to learn how to act optimally within an environment. Learning in this instance, refers to the agent interacting with the environment, getting feedback, and using that feedback to learn optimal behaviors. 

\section{Overview}
Suppose we have an environment $E$ that is in state $s_t \in \mathcal{S}$ at time step $t$ and an agent who can take actions $a_t \in \mathcal{A}$ that effect the current state $s_t$ of the environment $E$. The agent then recieves a reward $R_t$ for the action $a_t$ it took in state $s_t$. The goal of the agent is to learn a policy $\pi$ that maps states to actions such that the agent maximizes the expected sum of rewards over time.


\newpage

% ------------------------------------------------------------------------------
% Reference and Cited Works
% ------------------------------------------------------------------------------

\bibliographystyle{IEEEtran}
\bibliography{References.bib}

% ------------------------------------------------------------------------------

\end{document}
